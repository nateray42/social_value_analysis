#!/usr/bin/env python
# coding: utf-8
# %%
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# %%
import requests
import re
import datetime
import pandas as pd
import sys

sys.path.append('..//_system')
from basic_system import query_yes_no

from bs4 import BeautifulSoup
from tqdm import notebook


# %%
def main_search(agent, keyword, pages, tf):
    column_names = ['agent', 'link', 'title', 'article']
    df = pd.DataFrame(columns = column_names)
    index = df.index
    index.name = keyword
    
    if agent == 'í•œê²¨ë ˆ':
        df = hani_search(keyword, pages)
    elif agent == 'ì¤‘ì•™':
        df = joins_search(keyword, pages)
    elif agent == 'í•œê²½':
        df = hnky_search(keyword, pages)
    else:
        print('ë‹¤ì‹œ ì…ë ¥í•´ì¤˜~')
    
    df['agent'] = agent
    save_func(df, agent) if tf else tf


# %%
#í•œê²¨ë ˆ
def hani_search(keyword, pages):
    
    url = 'http://search.hani.co.kr/Search?command=query&media=news&sort=s&period=year&'
    
    for page in notebook.tqdm(range(int(pages))):
        try:
            req_params = {
                'keyword': keyword,
                'pageseq': page
                }
            response = requests.get(url, params=req_params)
            html = response.text.strip()
            soup = BeautifulSoup(html, 'lxml')
            results = soup.select('ul.search-result-list li dt a')
            banned_list = ['ì¸ì‚¬', 'ì•Œë¦¼', '[']

            for link in results:
                d = {}
                d['Link'] = 'http:' + link.get('href')
                d['Title'] = link.text
                d['Article'] = hani_article(d['Link'])
                #íŠ¹ì • ê¸°ì‚¬ ì œì™¸
                if all(x not in link.text for x in banned_list):
                    df_length = len(df)
                    df.loc[df_length] = d
        except requests.exceptions.ConnectionError:
            r.status_code = "Connection refused"
                
    return df


# %%
#ì¤‘ì•™ì¼ë³´
def joins_search(keyword, pages):
    
    url = 'http://news.joins.com/Search/TotalNews?'

    for page in notebook.tqdm(range(int(pages))):
        try:
            req_params = {
                'keyword': 'ê¸°ì—… í™˜ê²½',
                'Page': page,
                'StartSearchDate': '2020.01.01',
                'SortType' : 'Accuracy',
                'SearchCategoryType': 'JoongangNews'
                }
            response = requests.get(url, params=req_params)
            html = response.text.strip()
            soup = BeautifulSoup(html, 'lxml')
            results = soup.select('ul.list_default li h2 a')
            banned_list = ['[',']']

            for link in results:
                d = {}
                d['Link'] = link.get('href')
                d['Title'] = link.text
                d['Article'] = joins_article(d['Link'])
                #íŠ¹ì • ê¸°ì‚¬ ì œì™¸
                if all(x not in link.text for x in banned_list):
                    df_length = len(df)
                    df.loc[df_length] = d
        except requests.exceptions.ConnectionError:
            r.status_code = "Connection refused"

        
    return df


# %%
#í•œê²½
def hnky_search(keyword, pages):
    
    url = 'http://search.hankyung.com/apps.frm/search.news?&sort=RANK%2FDESC%2CDATE%2FDESC&'
    
    for page in notebook.tqdm(range(int(pages))):
        try:
            req_params = {
                'query': keyword,
                'page': page,
                'period': 'YEAR'
                }
            response = requests.get(url, params=req_params)
            html = response.text.strip()
            soup = BeautifulSoup(html, 'lxml')
            results = soup.select('div.txt_wrap > a')
            banned_list = ['ì˜ êµ­ê°', 'ì‹ ì„¤ë²•ì¸', 'ì¢…í•©']

            for link in results :
                if 'www.hankyung.com/' in link.get('href'):
                    d = {}
                    d['Link'] = link.get('href')
                    d['Title'] = clean_text(link.text)
                    d['Article'] = hnky_article(d['Link'])
                    #íŠ¹ì • ê¸°ì‚¬ ì œì™¸
                    if all(x not in link.text for x in banned_list):
                        df_length = len(df)
                        df.loc[df_length] = d
        except requests.exceptions.ConnectionError:
            r.status_code = "Connection refused"
        
    return df


# %%
def hani_article(url):
    response = requests.get(url, verify=False)
    html = response.text.strip()
    soup = BeautifulSoup(html, 'lxml')
    
    # í•œê²¨ë¡€ ê¸°ì‚¬ í¬ë¡¤ë§
    raw = soup.select('div.article-text div.text')[0]
    if raw.find('strong') is not None:
        unwanted = raw.find('strong')
        unwanted.extract()
        
    article = raw.text.strip()
    article = clean_text(article)
    return article


# %%
def joins_article(url):
    response = requests.get(url, verify=False)
    html = response.text.strip()
    soup = BeautifulSoup(html, 'lxml')
    
    # ì¤‘ì•™ì¼ë³´ ê¸°ì‚¬ í¬ë¡¤ë§
    raw = soup.select('div.article_body')[0]
    if raw.find('strong') is not None:
        unwanted = raw.find('strong')
        unwanted.extract()
        
    article = raw.text.strip()
    article = clean_text(article)
    return article


# %%
def hnky_article(url):
    response = requests.get(url, verify=False)
    html = response.text.strip()
    soup = BeautifulSoup(html, 'lxml')
    
    # í•œê²½ ê¸°ì‚¬ í¬ë¡¤ë§
    raw = soup.select('#articletxt')[0]
    if raw.find('strong') is not None:
        unwanted = raw.find('strong')
        unwanted.extract()
        
    article = raw.text.strip()
    article = clean_text(article)
    return article

# %%
# def agent_selector(agent):
#     dict agent_list:
#         # í•œê²¨ë ˆ ì‹ ë¬¸ì‚¬ ê²€ìƒ‰ URL
#         return url


# %%
def clean_text(text):
    cleaned_text = re.sub('[a-zA-Z]', '', text)
    cleaned_text = re.sub('[\{\}\[\]\/?.,;:|\)*~`!^Â·\-_+<>â–¶â–½â–³â–²âŠ™â™¡â—€â—†â—ˆâ€»â—‡â– â—â”â—â—‹@\#$%&\\\=\(\'\"â“’(\n)(\t)â€˜â€™â€œâ€ã€ƒ]', ' ', cleaned_text)
    cleaned_text = re.sub('ã¢ãˆœ', ' ', cleaned_text)
    cleaned_text = cleaned_text.replace("ğŸ‡²\u200bğŸ‡®\u200bğŸ‡±\u200bğŸ‡±\u200bğŸ‡®\u200bğŸ‡ª\u200b", "")
    cleaned_text = cleaned_text.replace("í•œêµ­ê²½ì œTV, ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€", "")
    cleaned_text = cleaned_text.replace("ì´ë¯¸ì§€ë¥¼ ëˆ„ë¥´ë©´ í¬ê²Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤", "")
    output = ' '.join(cleaned_text.split())
    return output


# %%
def save_func(df, agent):
    dt = datetime.datetime.now()
    date = dt.strftime('%Y_%m_%d_%H%M%S')
    title = df.index.name
    df.to_csv('../_data/' + '[' + agent + ']' + date + '[' + title + ']' + '.csv')


# %%
if __name__ == '__main__':
    agent = input('[!] ì–¸ë¡ ì‚¬ ì…ë ¥ : ')
    keyword = input('[!] ê²€ìƒ‰ì–´ ì…ë ¥ : ')
    pages = input('[!] ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜ : ')
    save_tf = '[!] csvë¡œ ì €ì¥í• ë˜? : '
    tf = query_yes_no(save_tf)
    main_search(agent, keyword, pages, tf)
    print('ì‘ì—… ì™„ë£Œí–ˆì–´~')

# %%
