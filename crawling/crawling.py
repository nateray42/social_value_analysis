#!/usr/bin/env python
# coding: utf-8
# %%
#https ì˜¤ë¥˜ ë¬´ì‹œ
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# %%
import requests
import re
import datetime
import pandas as pd
import sys

sys.path.append('..//_system')
from basic_system import query_yes_no

from bs4 import BeautifulSoup
from tqdm import notebook


# %%
#ë©”ì¸ ê²€ìƒ‰ í´ë˜ìŠ¤
class Main_search:
    def __init__(self, keyword, page):
        self.keyword = keyword
        self.page = page
        self.whitelist = ''
        
    def search(self):
        column_names = ['agent', 'link', 'title', 'article']
        df = pd.DataFrame(columns = column_names)
        
        for page in notebook.tqdm(range(int(self.page))):
            try:
                parameter = self.parameter
                parameter[(list(parameter)[0])] = page
                response = requests.get(self.url, params = parameter)
                html = response.text.strip()
                soup = BeautifulSoup(html, 'lxml')
                results = soup.select(self.title_selector)

                for link in results:
                    
                    d = {}
                    d['agent'] = self.agent
                    d['link'] = link.get('href')
                    if self.whitelist in link.get('href'):
                        if 'http' not in d['link']:
                            d['link'] = 'http:' + d['link']
                        d['title'] = self.clean_text(link.text)
                        d['article'] = self.find_article(d['link'])

                        if all(x not in link.text for x in self.banned_list):
                            df_length = len(df)
                            df.loc[df_length] = d
            except requests.exceptions.ConnectionError:
                r.status_code = "Connection refused"
        
        self.df = df
        return df
                
    def find_article(self, url):
        response = requests.get(url, verify=False)
        html = response.text.strip()
        soup = BeautifulSoup(html, 'lxml')
        raw = soup.select(self.text_selector)[0]
        if raw.find('strong') is not None:
            unwanted = raw.find('strong')
            unwanted.extract()

        self.text = raw.text.strip()
        self.article = self.clean_text(self.text)
        return self.article
    
    def clean_text(self, text):
        cleaned_text = re.sub('[a-zA-Z]', '', text)
        cleaned_text = re.sub('[?!]', ' . ', cleaned_text)
        cleaned_text = re.sub('[\{\}\[\]\/,;:|\)*~`^Â·\-_+<>â–¶â–½â–³â–²âŠ™â™¡â—€â—†â—ˆâ€»â—‡â– â—â”â—â—‹@\#$%&\\\=\(\'\"â“’(\n)(\t)â€˜â€™â€œâ€ã€ƒ]', ' ', cleaned_text)
        cleaned_text = re.sub('ã¢ãˆœ', ' ', cleaned_text)
        cleaned_text = cleaned_text.replace("ğŸ‡²\u200bğŸ‡®\u200bğŸ‡±\u200bğŸ‡±\u200bğŸ‡®\u200bğŸ‡ª\u200b", "")
        cleaned_text = cleaned_text.replace("í•œêµ­ê²½ì œTV, ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€", "")
        cleaned_text = cleaned_text.replace("ì´ë¯¸ì§€ë¥¼ ëˆ„ë¥´ë©´ í¬ê²Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤", "")
        output = ' '.join(cleaned_text.split())
        return output
    
    def save_func(self):
        dt = datetime.datetime.now()
        date = dt.strftime('%Y_%m_%d_%H%M%S')
        self.df.to_csv('../_data/' + '[' + self.agent + ']' + date + '[' + self.keyword + ']'\
                  + '.csv')
        
    def show_data(self):
        print(self.df.head())


# %%
#í•œê²¨ë ˆ ê²€ìƒ‰ í´ë˜ìŠ¤
class Hani_search(Main_search):
    def __init__(self, keyword, page):
        super(Hani_search, self).__init__(keyword, page)
        self.agent = 'í•œê²¨ë ˆ'
        self.url = 'http://search.hani.co.kr/Search?command=query&media=news&sort=s&period=year&'
        self.parameter = {
                    'keyword': keyword,
                    'pageseq': page
                    }
        self.title_selector = 'ul.search-result-list li dt a'
        self.text_selector = 'div.article-text div.text'
        self.banned_list = ['ì¸ì‚¬', 'ì•Œë¦¼', '[', 'ê·¸ë¦¼íŒ', 'ì†Œì‹']
        
    def search(self):
        super(Hani_search, self).search()


# %%
#ì¤‘ì•™ì¼ë³´ ê²€ìƒ‰ í´ë˜ìŠ¤
class Joins_search(Main_search):
    def __init__(self, keyword, page):
        super(Joins_search, self).__init__(keyword, page)
        self.agent = 'ì¤‘ì•™'
        self.url = 'http://news.joins.com/Search/TotalNews?'
        self.parameter = {
                'keyword': keyword,
                'Page': page,
                'StartSearchDate': '2020.01.01',
                'SortType' : 'Accuracy',
                'SearchCategoryType': 'JoongangNews'
                }
        self.title_selector = 'ul.list_default li h2 a'
        self.text_selector = 'div.article_body'
        self.banned_list = ['[',']']
    def search(self):
        super(Joins_search, self).search()


# %%
#í•œê²½ ê²€ìƒ‰ í´ë˜ìŠ¤
class Hnky_search(Main_search):
    def __init__(self, keyword, page):
        super(Hnky_search, self).__init__(keyword, page)
        self.whitelist = 'www.hankyung.com/'
        self.agent = 'í•œê²½'
        self.url = 'http://search.hankyung.com/apps.frm/search.news?&sort=RANK%2FDESC%2CDATE%2FDESC&'
        self.parameter = {
                'query': keyword,
                'page': page,
                'period': 'YEAR'
                }
        self.title_selector = 'div.txt_wrap > a'
        self.text_selector = '#articletxt'
        self.banned_list = ['ì˜ êµ­ê°', 'ì‹ ì„¤ë²•ì¸', 'ì¢…í•©']
    def search(self):
        super(Hnky_search, self).search()


# %%
def main_func(agent, keyword, pages, tf):
    
    if agent == 'í•œê²¨ë ˆ':
        func = Hani_search(keyword, pages)
    elif agent == 'ì¤‘ì•™':
        func = Joins_search(keyword, pages)
    elif agent == 'í•œê²½':
        func = Hnky_search(keyword, pages)
    else:
        print('ë‹¤ì‹œ ì…ë ¥í•´ì¤˜~')
        
    df = func.search() 
    func.save_func() if tf else tf
    print('ì‘ì—… ì™„ë£Œí–ˆì–´~')



# %%
if __name__ == '__main__':
    agent = input('[!] ì–¸ë¡ ì‚¬ ì…ë ¥ : ')
    keyword = input('[!] ê²€ìƒ‰ì–´ ì…ë ¥ : ')
    pages = input('[!] ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜ : ')
    save_tf = '[!] csvë¡œ ì €ì¥í• ë˜? : '
    tf = query_yes_no(save_tf)
    main_func(agent, keyword, pages, tf)
    

# %%
